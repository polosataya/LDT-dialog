# streamlit run app.py
import streamlit as st
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from catboost import CatBoostClassifier, Pool
import spacy

import re

st.set_page_config(
    page_title="–ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–¥–∞—é—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤",
    page_icon="üìù", layout="wide",
    initial_sidebar_state="expanded",
    menu_items={'Get Help': None,'Report a bug': None,'About': None})

hide_streamlit_style = """<style>#MainMenu {visibility: hidden;}footer {visibility: hidden;}</style>"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

############################################################################
# —Ñ—É–Ω–∫—Ü–∏–∏
############################################################################
lemmatizer = spacy.load('ru_core_news_md', disable = ['parser', 'ner'])
stopwords_nltk=[]

def full_clean(s):
    #–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∫ –ø–æ–¥–∞—á–µ –≤ –º–æ–¥–µ–ª—å
    s=re.sub(r"[^a-zA-Z–∞-—è–ê-–Ø–π–ô#]", " ", s)
    s = s.lower()
    s = re.sub(" +", " ", s) #–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 1 –ø—Ä–æ–±–µ–ª
    text = " ".join([token.lemma_ for token in lemmatizer(s) if token.lemma_ not in stopwords_nltk])
    
    return text

def tfidf_featuring(tfidf, df):   
    '''–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤'''
    X_tfidf = tfidf.transform(df)
    feature_names = tfidf.get_feature_names_out()
    X_tfidf = pd.DataFrame(X_tfidf.toarray(), columns = feature_names, index = df.index)
    
    return X_tfidf

def get_params(data, start, emotional):
    # –Ω–æ–º–µ—Ä –¥–∏a–ª–æ–≥–∞ –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    nums=[]
    num = start
    for i, row in data.iterrows():
        if row["‚Ññ —Å–æ–æ–±—â–µ–Ω–∏—è"]==1:
            num+=1
        nums.append(num)
    data['‚Ññ –¥–∏–∞–ª–æ–≥–∞']=nums
    data['emotional']=emotional
    return data

def create_texts(data):
    # —Ç–µ–∫—Å—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    texts=[]
    string = ''
    for i, row in data.iterrows():
        if row["‚Ññ —Å–æ–æ–±—â–µ–Ω–∏—è"]==1:
            if string != '':
                texts.append(string)
                string = str(row["–¢–µ–∫—Å—Ç"])+' '
            else:
                string += str(row["–¢–µ–∫—Å—Ç"])+' '
        else:
            if row["–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"]=='in':
                string += str(row["–¢–µ–∫—Å—Ç"])+' '
    texts.append(string)
    return texts

@st.cache_data
def load_file(falepath):
    '''–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞'''
    df = pd.read_excel(falepath, engine='openpyxl') #sheet_name='valid', 
    df = get_params(df, 0, "unknow")
    data = df[['‚Ññ –¥–∏–∞–ª–æ–≥–∞', 'emotional']].drop_duplicates()
    data['texts']=create_texts(df)
    data['–æ—Ç–∫–∞–∑ –æ—Ç –ø—Ä–æ–¥—É–∫—Ç–æ–≤']=0
    data['–∂–∞–ª–æ–±—ã']=0
    data['–ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å']=0
    data['–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ']=0
    data['—É—Ç–µ—Ä—è/–∫—Ä–∞–∂–∞ –∫–∞—Ä—Ç—ã']=0
    data['clean'] = data['texts'].apply(lambda x: full_clean(x))
    return df, data

@st.cache_data
def load_tfidf(falepath):
    '''–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞'''
    tfidf = TfidfVectorizer()
    tfidf = joblib.load(falepath)
    return tfidf

@st.cache_data
def load_model1(falepath):
    '''–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏'''
    model_1 = CatBoostClassifier()
    model_1.load_model(falepath)
    return model_1 

@st.cache_data
def load_model1(falepath):
    '''–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å—Ç–æ–ø —Ç–µ–º'''
    model_2 = CatBoostClassifier()
    model_2.load_model(falepath)
    return model_2

############################################################################
# –∑–∞–≥—Ä—É–∑–∫–∞
############################################################################

stop_theme = ['–æ—Ç–∫–∞–∑ –æ—Ç –ø—Ä–æ–¥—É–∫—Ç–æ–≤', '–∂–∞–ª–æ–±—ã', '–ø—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å', '–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ', '—É—Ç–µ—Ä—è/–∫—Ä–∞–∂–∞ –∫–∞—Ä—Ç—ã']

#–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
tfidf = joblib.load('model/tfidf.pkl')

#—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
model_1 = load_model1('model/model1.cbm')

#—Å—Ç–æ–ø —Ç–µ–º—ã
model_2 = load_model1('model/model2.cbm')


#df, data=load_file('data/–ì–∞–∑–ø—Ä–æ–º_valid.xlsx')

############################################################################
# –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
############################################################################

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.title("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
upload_button = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏", type=["xlsx", "csv"])

if upload_button is not None:
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –∫–Ω–æ–ø–∫–∞ –Ω–∞–∂–∞—Ç–∞
    df, data = load_file(upload_button)
else:
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –∫–Ω–æ–ø–∫–∞ –Ω–µ –Ω–∞–∂–∞—Ç–∞
    df, data = load_file('data/–ì–∞–∑–ø—Ä–æ–º_valid.xlsx')

X_tfidf = tfidf_featuring(tfidf, data['clean'])

valid_predict1 = model_1.predict(X_tfidf)
valid_predict2 = model_2.predict(X_tfidf)

t = pd.DataFrame(valid_predict2, columns=stop_theme, index = list(data['‚Ññ –¥–∏–∞–ª–æ–≥–∞']))
t['–°—Ç–æ–ø —Ç–µ–º—ã']=t.astype(int).dot(t.columns+',').str[:-1]
t['‚Ññ –¥–∏–∞–ª–æ–≥–∞']=t.index
t['–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å']=valid_predict1
t['–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å']=t['–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å'].map({1:'positive', 0: "neutral", -1:'negative'})

# –°–æ–∑–¥–∞–µ–º –ø–æ–ª–∑—É–Ω–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞ –¥–∏–∞–ª–æ–≥–∞
selected_dialog_index = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–º–µ—Ä –¥–∏–∞–ª–æ–≥–∞", 0, len(df['‚Ññ –¥–∏–∞–ª–æ–≥–∞'].unique()) - 1)

# –ü–æ–ª—É—á–∞–µ–º –Ω–æ–º–µ—Ä –¥–∏–∞–ª–æ–≥–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É
selected_dialog = df['‚Ññ –¥–∏–∞–ª–æ–≥–∞'].unique()[selected_dialog_index]

# –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞
dialog_data = df[df['‚Ññ –¥–∏–∞–ª–æ–≥–∞'] == selected_dialog]

# –£–ª—É—á—à–∞–µ–º —Å—Ç–∏–ª—å –≤—ã–≤–æ–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –¥–∏–∞–ª–æ–≥–µ
st.subheader("–î–∏–∞–ª–æ–≥")
st.table(df[df['‚Ññ –¥–∏–∞–ª–æ–≥–∞'] == selected_dialog][['‚Ññ —Å–æ–æ–±—â–µ–Ω–∏—è', '–¢–µ–∫—Å—Ç', '–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ']].set_index("‚Ññ —Å–æ–æ–±—â–µ–Ω–∏—è"))

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
selected_predictions = t[t['‚Ññ –¥–∏–∞–ª–æ–≥–∞'] == selected_dialog]

# –£–ª—É—á—à–∞–µ–º —Å—Ç–∏–ª—å –≤—ã–≤–æ–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–≥–Ω–æ–∑–µ
st.subheader("–ü—Ä–æ–≥–Ω–æ–∑")

# –û—Ç–¥–µ–ª—å–Ω–æ –≤—ã–≤–æ–¥–∏–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
tonality = selected_predictions['–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å'].iloc[0]
st.write(f"**–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {tonality}")

# –û—Ç–¥–µ–ª—å–Ω–æ –≤—ã–≤–æ–¥–∏–º —Å—Ç–æ–ø —Ç–µ–º—ã –∏ –∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è
stop_themes = selected_predictions['–°—Ç–æ–ø —Ç–µ–º—ã'].iloc[0].split(',')
stop_themes_str = ', '.join(stop_themes)

st.write(f"**–°—Ç–æ–ø —Ç–µ–º—ã:** {stop_themes_str}")